---
title: "Decision tree regressor - Implementation"
output: html_notebook
---
```{r paged.print=FALSE}
# loading the data 
df  =  read.csv('/home/thomaskutty/Gitrepo/Machine-Learning-with-R/data_fold/forestfires.csv')
head(df)
```

```{r}
# splitting the dataframe into train and test 
library(caTools)
# help(caTools)
# help("sample.split")
sample = sample.split(df$ISI,SplitRatio = 0.8)
df.train = subset(df, sample == TRUE)
df.test = subset(df, sample == FALSE)
print(dim(df.train))
print(dim(df.test))
```

```{r}
# Checking the null values of each feature 
colSums(is.na(df.train))
```
Note: We see that there is no missing values in this training dataset 
```{r}
# gettting the structure of the train datset 
str(df.train)
```
Note: Only two features ( month and day) are character datatype all others are numerical.


```{r message=FALSE, warning=FALSE}
library(ggplot2)
ggplot(data = df.train, aes(df.train$ISI)) + geom_histogram(aes (fill = 'lightblue'))+ theme_minimal()
```
NOte: We can see that the target follows normal distribution , but we also found that there is some outlier in the right end of the graph. 

```{r message=FALSE, warning=FALSE}
ggplot(data = df.train, aes(df.train$month)) + geom_bar(aes (fill = 'lightblue'))+ theme_minimal()
```
Note: we see that there ar much data points for august and september months.

```{r message=FALSE, warning=FALSE}
ggplot(data = df.train, aes(df.train$X)) + geom_boxplot(aes (fill = 'blue
                                                             ')) + theme_minimal()
```
we can see that there is no outliers in the x feature 

```{r message=FALSE, warning=FALSE}
ggplot(data = df.train, aes(df.train$Y)) + geom_boxplot() + theme_minimal()
```
y feature has outliers ( 3 )
```{r message=FALSE, warning=FALSE}
ggplot(data = df.train, aes(df.train$RH)) + geom_boxplot() + theme_minimal()

```
For RH feature also we detected outliers in the right tail. 


```{r message=FALSE, warning=FALSE}
attach(df)
library(rpart)
library(rpart.plot)
df.fit = rpart(df.train$ISI ~. , method = "anova", data = df.train)
df.fit
```

```{r}
rpart.plot(df.fit, box.col = c('yellow', 'blue'), main = "Regression for ISI")
```
we can infer that the if ffmc is less than 88 than 15% 
```{r}
par(mfrow = c(1,2))
rsq.rpart(df.fit)
```
we have trained our tree -fit using all the variables in the dataset. Above we see how the function rpart creates multiple tree and compares all the result to select the tree that contains least error and provides us with better model with higher accuracy and finally use it. 
Above we can see the graphical representation of the different trees created and how the best tree that gives less error is choosen. 
As the number of splits increases the accuracy ie , R -square increases and the relative error decreases. 
We predict the isi vlaue of the test dataset using the model which we trained by the train datset. 

```{r}
predictions  = predict(df.fit, df.test, method = "anova")
results = cbind(predictions, df.test$ISI)
colnames(results) = c('pred', 'real')

results = as.data.frame(results)

sse = sum((results$pred - results$real)^2)

sst = sum((mean(df$ISI)  - results$real)^2)

r2 = 1 - (sse/sst)

print(paste(sse, "is the SSE"))
```
```{r}
print(paste(sst, 'is the sst'))
```
```{r}
print(paste(r2, 'is the accuracy'))
```

Interpretation : Sum of squares (sst) is the squared differences between the observed dependent variable and its mean. it is th edispersion of the observed variables around the mean its like the variance in descriptive statictics Sum of squares due to regression, or ssr is the sum of the differences betweent the predicted value and the mean of the dependent variable.
Here the accuracy is 0.73011 ie, the best model generated by using the data set was 73.01 % accurate in predicting the sepal length in our test dataset. 
=================================================================================